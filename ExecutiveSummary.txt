Instructions:

I am providing .zip folders for the server and the client code. Please unzip each. If you are able to see my code on
nodes 1 through 6 already then skip this step. Otherwise, upload the server package to nodes 2-6 and upload the client package to
node 1. The server IP addresses are already hard-coded. For each node, run the following command:

python3 server/Server.py -p<port>

It is critical that each node is running on the same port. This was the simplest way for me to implement the project.
Once all servers are running (a message will be displayed stating the server is listening on the port specified as well
as its own IP address), run the client code by running the command:

python3 client/TCPClient.py -s<server_address> -p<port>

An alternative method for installation is to clone the GitHub repositories to the nodes listed above and then ‘cd’ into
the DistributedKeyStore directory and run the above commands.

https://github.com/jonahkh/DistributedKeyStore

Assignment Overview:

The purpose of this assignment was to expose us to many of the issues that arise when working with a distributed system.
This was the first time we were required to work with more than one running server instance, so the challenges were quite
different than running with only one server. Not only did we have to be able to handle multiple incoming requests from
one to many clients, we had to be able to handle incoming requests from other servers implementing the two-phased commit.

Technical Impression:

For the requirements, more description on what was expected of us in terms of our implementation would have been very
helpful. I ended up guessing regarding certain issues such as should each node acquire the resource lock during phase one
of the 2PC or from the commit message. Areas I struggled in were handling an unresponsive server. Most of my issues were
based on not fully understanding how the socket library is implemented for Python. For example, when trying to establish
a connection with another server, the connection function call does not have its own retry built in, it only has a timeout.
Therefore I had to implement my own retry mechanism to handle an unresponsive server, and the whole request would abort
after a total of 5 seconds. Another issue was figuring out how to call all of the servers asynchronously and determine
when and if they had all been completed separately. For this, I took advantage of a built-in Python library called
ThreadPoolExecutor which allows you to assign a function and arguments to a task that gets executed asynchronously and
you can specify exactly how many different threads would be allowed to execute. A timeout could be set and each task returns
a 'future' object which is how you can track the status of the asynchronous call. Overall this assignment was very fun
to implement and I learned a lot more about sockets, multi-threading, asynchronous calls, and the Python library in general.
I've only been working with Python for the duration of this quarter so jumping into a project like this in a new language
has been very interesting and rewarding.


If I were to do this project differently, I definitely would have just used RPC. This would have allowed me to eliminate
my socket code all together. The only reason I didn't was because I had thought I understood TCP the most.


Team Discussion:


I worked on this project solo because my work schedule was not working out with my team and there was no communication.
I thought it would make more sense for me to complete it on my own with the benefit of learning more.


Performance Analysis:

For the results, I found that the single server TCP protocol performed much faster than the 2PC. As you can see in the
graph, the average time for a PUT or DELETE was around 20ms for 2PC versus 2ms for TCP. The overall average throughput for
TCP was 89.5 transactions a second and 2PC was at 20.3 transactions a second. Clearly the 2PC protocol took significantly
more time than the single-server TCP. GET operations were not affected since no inter-server communication was done.

Sources:


Any code replication was based on examples from https://wiki.python.org