Instructions:

I am providing .zip folders for the server and the client code. Please unzip each. If you are able to see my code on
nodes 1 through 6 already then skip this step. Otherwise, upload the server package to nodes 2-6 and upload the client package to
node 1. The server IP addresses are already hard-coded. For each node, run the following command:

python3 server/Server.py -p<port>

It is critical that each node is running on the same port. This was the simplest way for me to implement the project.
Once all servers are running (a message will be displayed stating the server is listening on the port specified as well
as its own IP address), run the client code by running the command:

python3 client/TCPClient.py -s<server_address> -p<port>

An alternative method for installation is to clone the GitHub repositories to the nodes listed above and then ‘cd’ into
the DistributedKeyStore directory and run the above commands.

https://github.com/jonahkh/DistributedKeyStore

Assignment Overview:

The purpose of this assignment was to expose us to the issues that arise when building distributed, replicated system.
The two-phase commit was okay assuming that none of the nodes would fail, but would fail if some of them did. With Paxos,
we just have to have a majority of nodes respond in order to commit the message which gives it obvious advantages.

Technical Impression:

This assignment was definitely more difficult than the previous ones. A lot of the difficulty came down to how to actually
design the three-phase commit. There were different approaches i was thinking about, such as having threads only execute
one of the phases at a time. Instead, I went with having one thread execute a single all three stages at once. This has
the potential for livelock situations since there is a limited number of threads so I implemented timeouts in the event
that there was no response from the leader. I did not use a leader-election algorithm, I just assumed that whichever node
received the request would be the leader for that particular operation. For the second requirement, I just had each acceptor
choose a random number between 1 and 100. If it was the 'magic number' (51), then the acceptor would just return with no
response. Another way to test out this functionality is by starting up all but 2 of the nodes and the client requests
will still be fulfilled. I found it difficult to implement the accept phase because of the highest value issue. I just
ended up mapping each response to the count representing how many times that value has been received. The value was set
as the key, value, operation dictionary object.

Team Discussion:


I worked on this project solo because my work schedule was not working out with my team and there was no communication.
I thought it would make more sense for me to complete it on my own with the benefit of learning more.


Performance Analysis:

Paxos performed only slightly worse than 2PC with an average throughput of 14.13 transactions/second, 2PC was at
20.3 transactions/second, and TCP was 89.5 transactions/second. This reduced throughput was a result of the extra phase
needed to commit. I also implemented the algorithm with timeouts/retries which contribute to the overall operation time.
Gets are consistently the same for all three protocols.

Sources:

Any code replication was based on examples from https://wiki.python.org